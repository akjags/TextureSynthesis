{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy\n",
    "from pt_tex_synth import *\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below (don't worry about what they're doing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### HIDE \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# desired size of the output image\n",
    "imsize = 256 if torch.cuda.is_available() else 128  # use small size if no gpu\n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),  # scale imported image\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "# Plotting and saving functions.\n",
    "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    #plt.pause(0.001) # pause a bit so that plots are updated\n",
    "\n",
    "def imsave(tensor, savepath=None):\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    image.save(savepath)  \n",
    "\n",
    "# Load the VGG19 weights and model architecture.\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNNs) are a class of models that have been extremely successful at the problem of **object recognition**. In this activity, we will first explore why this problem has been so difficult, and then we will see how convolutional neural networks have been able to solve this problem. Next, we will dig into the internals of a convolutional neural network to try and understand why they have been so successful. Finally, we will try to compare these artificial neural networks with the natural neural networks in our head -- i.e. your *brain*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Visual Object Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of why this problem is so challenging, take a look at the two images below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img1 = np.load('penguin.jpg');\n",
    "img2 = np.load('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Visualizing the internal layers of a convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit the cell below to specify which original image and which layer, then run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Specify which image, which layer, and other important synthesis parameters.\n",
    "orig_im_name = \"tulips.jpg\"\n",
    "layer = 'pool1' # Choose either pool1, pool2, pool3 or pool4.\n",
    "num_steps = 1000\n",
    "stim_dir = '/home/users/akshayj/TextureSynthesis/stimuli/textures/orig_color' # change to match stim directory.\n",
    "out_dir = 'output' # Directory to save the image in (relative to stim_dir)\n",
    "\n",
    "# Uncomment the line below to see a list of all the original images you can play around with.\n",
    "#print(os.listdir(stim_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n",
      "Optimizing..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt_tex_synth.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
      "pt_tex_synth.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std = torch.tensor(std).view(-1, 1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #50 style loss: 408.615448\n",
      "Step #100 style loss: 16.908377\n",
      "Step #150 style loss: 2.995599\n",
      "Step #200 style loss: 1.498470\n",
      "Step #250 style loss: 0.857577\n",
      "Step #300 style loss: 114.373291\n",
      "Step #350 style loss: 0.464300\n",
      "Step #400 style loss: 0.163760\n",
      "Step #450 style loss: 0.047246\n",
      "Step #500 style loss: 0.020024\n",
      "Step #550 style loss: 0.010989\n",
      "Step #600 style loss: 0.007344\n",
      "Step #650 style loss: 0.005336\n",
      "Step #700 style loss: 0.004243\n",
      "Step #750 style loss: 0.003714\n",
      "Step #800 style loss: 0.002710\n",
      "Step #850 style loss: 0.002340\n",
      "Step #900 style loss: 0.001719\n",
      "Step #950 style loss: 2829.497803\n",
      "Step #1000 style loss: 47.757305\n",
      "Done synthesizing! Took 31.1841909885 seconds\n"
     ]
    }
   ],
   "source": [
    "# After specifying above, run this cell to start the texture synthesis!\n",
    "saveName = '{}_{}.png'.format(layer, orig_im_name.split('.')[0]) # Save as: e.g. pool2_cherries.png\n",
    "\n",
    "# Specify the texture image to match\n",
    "orig_img = image_loader(stim_dir + '/' + orig_im_name)\n",
    "\n",
    "# Randomly initialize white noise input image\n",
    "init_img = torch.randn(orig_img.data.size(), device=device)\n",
    "\n",
    "# Make the output directory, if it doesn't already exist.\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "# Select layer\n",
    "all_layers = ['conv1_1', 'pool1', 'pool2', 'pool3', 'pool4'];\n",
    "this_layers = all_layers[:all_layers.index(layer)+1]\n",
    "\n",
    "# Run!\n",
    "t = time.time()\n",
    "output_img = run_texture_synthesis(cnn, cnn_normalization_mean, cnn_normalization_std, orig_img, \n",
    "                                    init_img, num_steps=num_steps, style_layers=this_layers, \n",
    "                                    saveLoc=[stim_dir + '/' + out_dir, saveName])\n",
    "\n",
    "# Save the image you just generated\n",
    "imsave(output_img, stim_dir + '/' + out_dir + '/' + saveName);\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('Done synthesizing! Took {} seconds'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cell below to plot the texture you just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orig_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-db998d217415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mplot_textures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_im_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstim_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# First plot the original image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'orig_img' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_textures(orig_im_name, layers=[], orig_img = orig_img, save_dir = stim_dir + '/' + out_dir):\n",
    "    \n",
    "    plt.figure(figsize=(18,10))\n",
    "\n",
    "    # First plot the original image\n",
    "    plt.subplot(1,len(layers)+1,1);\n",
    "    imshow(orig_img, title='Original Image')\n",
    "    plt.axis('off');\n",
    "    \n",
    "    # then plot everything else.\n",
    "    for i in range(2, len(layers)+2):\n",
    "        img = image_loader('{}/{}_{}.png'.format(save_dir,layers[i-2],orig_im_name))\n",
    "        plt.subplot(1,len(layers)+1,i);\n",
    "        imshow(img, title=layers[i-2])\n",
    "        plt.axis('off');\n",
    "\n",
    "## you have to specify the image name and which layers to plot\n",
    "plot_textures('tulips', ['pool1', 'pool2', 'pool4'], orig_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Analysis of Behavior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
